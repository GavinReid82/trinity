{% extends "layout.html" %}

{% block title %}Delivering a prepared talk{% endblock %}

{% block content %}
<h1 class="page-title">Delivering a prepared talk <img src="{{ url_for('static', filename='images/speaking.jpg') }}" alt="Speaking icon" class="title-icon"></h1>

<div class="container mt-4">
    <div id="taskContainer">
        <!-- Video player -->
        <video id="taskVideo" class="w-100 mb-3" controls>
            <source id="videoSource" src="" type="video/mp4">
            Your browser does not support the video element.
        </video>

        <!-- Recording interface (hidden initially) -->
        <div id="recordingInterface" class="text-center mb-3" style="display: none;">
            <button id="submitButton" class="btn btn-primary">Submit Recording Early</button>
        </div>

        <!-- Progress indicator (present but hidden) -->
        <div class="progress mb-3" style="display: none;">
            <div id="progressBar" class="progress-bar" role="progressbar" style="width: 0%" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100"></div>
        </div>
    </div>
</div>

<script>
const videos = {
    intro: '/static/videos/task2/intro.mp4',
    speaking: '/static/videos/task2/speaking.mp4',
    followup: '/static/videos/task2/followup.mp4'
};

let currentStage = 'intro';
let mediaRecorder = null;
let audioChunks = [];
let recordingTimeout = null;
let recordingStartTimeout = null;
let isSubmitting = false;

const videoElement = document.getElementById('taskVideo');
const recordingInterface = document.getElementById('recordingInterface');
const submitButton = document.getElementById('submitButton');
const progressBar = document.getElementById('progressBar');

function updateProgress(stage) {
    const progress = {
        'intro': 33,
        'speaking': 66,
        'followup': 100
    };
    progressBar.style.width = `${progress[stage]}%`;
}

async function startRecording(duration) {
    try {
        console.log('Starting recording process...');
        recordingInterface.style.display = 'block';
        
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log('Microphone access granted');
        
        mediaRecorder = new MediaRecorder(stream);
        audioChunks = [];

        mediaRecorder.ondataavailable = (event) => {
            audioChunks.push(event.data);
        };

        mediaRecorder.onstop = async () => {
            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
            await submitRecording(audioBlob);
            stream.getTracks().forEach(track => track.stop());
        };

        mediaRecorder.start();
        console.log('Recording started');
        updateSubmitButton('reset');
        
        setTimeout(() => {
            recordingInterface.style.display = 'block';
            console.log('Recording interface display set to:', recordingInterface.style.display);
        }, 500);
        
        recordingTimeout = setTimeout(() => {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                updateSubmitButton('auto');
                mediaRecorder.stop();
            }
        }, duration * 1000);
    } catch (error) {
        console.error('Error starting recording:', error);
        alert('Failed to start recording. Please ensure microphone access is granted.');
        recordingInterface.style.display = 'block';
        submitButton.textContent = 'Microphone access denied. Please reload the page.';
        submitButton.disabled = true;
    }
}

async function submitRecording(audioBlob) {
    if (isSubmitting) return;
    isSubmitting = true;

    try {
        updateSubmitButton(currentStage === 'speaking' ? 'auto' : 'manual');
        
        const formData = new FormData();
        formData.append('audio_file', audioBlob);
        formData.append('stage', currentStage === 'speaking' ? 'main' : 'followup');
        formData.append('timestamp', Date.now());

        console.log(`Submitting ${currentStage} recording with stage: ${formData.get('stage')}`);

        const response = await fetch('/speaking/speaking_task_2_submit', {
            method: 'POST',
            body: formData,
            cache: 'no-cache'
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const data = await response.json();
        
        if (!data.success) {
            throw new Error('Submission was not successful');
        }

        console.log(`Successfully submitted ${currentStage} recording`);
        
        // Handle next steps based on current stage
        if (currentStage === 'speaking') {
            console.log('Moving to follow-up video...');
            await playNextVideo();
        } else if (currentStage === 'followup') {
            console.log('Redirecting to feedback page...');
            window.location.href = '/speaking/speaking_task_2_feedback';
        }
    } catch (error) {
        console.error('Error:', error);
        alert('There was an error submitting your recording. Please try again.');
        updateSubmitButton('reset');
    } finally {
        isSubmitting = false;
    }
}

async function playNextVideo() {
    // Clear any existing timeouts
    if (recordingStartTimeout) {
        console.log('Clearing previous recordingStartTimeout');
        clearTimeout(recordingStartTimeout);
        recordingStartTimeout = null; // Reset timer variable
    }
    if (recordingTimeout) {
        console.log('Clearing previous recordingTimeout');
        clearTimeout(recordingTimeout);
         recordingTimeout = null; // Reset timer variable
    }
     // Also stop any active recorder from the previous stage if it somehow didn't stop
    if (mediaRecorder && mediaRecorder.state === 'recording') {
        console.warn('Stopping active mediaRecorder before playing next video.');
        mediaRecorder.stop(); // This calls onstop, which then calls submitRecording
    }


    const stages = ['intro', 'speaking', 'followup'];
    const currentIndex = stages.indexOf(currentStage);
    const nextStage = stages[currentIndex + 1];

    if (nextStage) {
        console.log(`playNextVideo: Moving to ${nextStage} stage`);
        currentStage = nextStage;
        videoElement.src = videos[currentStage];

        // Make sure we play the video immediately
        try {
            console.log(`playNextVideo: Attempting to play ${nextStage} video...`);
            await videoElement.play();
            console.log(`playNextVideo: ${nextStage} video playing`);
        } catch (error) {
            console.error(`playNextVidedo: Error playing ${nextStage} video:`, error);
            // Try again after a short delay - Let's keep this robust play attempt
            setTimeout(() => videoElement.play().catch(e => console.error(`Retry play failed for ${nextStage}:`, e)), 500);
            // Removed alert as it might interrupt flow, rely on console for errors now
        }

        updateProgress(currentStage);

        // Hide recording interface and reset button state
        console.log('playNextVideo: Hiding recording interface.');
        recordingInterface.style.display = 'none';
        updateSubmitButton('reset');

        // Restore the original logic for scheduling recordings
        if (nextStage === 'speaking') {
            console.log('playNextVideo: Scheduling speaking recording to start in 6 seconds...');
            recordingStartTimeout = setTimeout(async () => {
                 console.log('recordingStartTimeout (speaking): Timer fired. Calling startRecording.');
                 await startRecording(120); // 2 minutes
            }, 6000);
             console.log('playNextVideo: recordingStartTimeout (speaking) set.');

        } else if (nextStage === 'followup') {
            console.log('playNextVideo: Scheduling followup recording to start in 7 seconds...');
             recordingStartTimeout = setTimeout(async () => {
                 console.log('recordingStartTimeout (followup): Timer fired. Calling startRecording.');
                 await startRecording(30); // 30 seconds
             }, 7000);
             console.log('playNextVideo: recordingStartTimeout (followup) set.');
        }
    } else {
         console.log('playNextVideo: No next stage found.');
    }
}

function updateSubmitButton(state) {
    const submitButton = document.getElementById('submitButton');
    
    switch(state) {
        case 'manual':
            submitButton.className = 'btn btn-warning';
            submitButton.textContent = 'Submitting response, please wait...';
            submitButton.disabled = true;
            break;
        case 'auto':
            submitButton.className = 'btn btn-warning';
            submitButton.textContent = 'Submitting response, please wait...';
            submitButton.disabled = true;
            break;
        case 'reset':
            submitButton.className = 'btn btn-primary';
            submitButton.textContent = 'Submit Recording Early';
            submitButton.disabled = false;
            break;
        default:
            submitButton.className = 'btn btn-primary';
            submitButton.textContent = 'Submit Recording Early';
            submitButton.disabled = false;
    }
}

// Event listeners
videoElement.addEventListener('ended', async () => {
    console.log(`Video ended. Current stage: ${currentStage}`);
    if (currentStage === 'intro') {
        // Original behavior: automatically move to the next video after intro ends
        await playNextVideo();
    } else if (currentStage === 'speaking') {
         // If speaking video ends AND recording is active, let the recording timeout/submission handle it
         if (mediaRecorder && mediaRecorder.state === 'recording') {
            console.log('Speaking video ended, waiting for recording timeout/submission.');
         } else {
             console.log('Speaking video ended, recording not active or already stopped.');
         }
    } else if (currentStage === 'followup') {
        console.log('Followup video ended.');
        // Submission handler should redirect, so no action needed here.
    }
});

submitButton.addEventListener('click', () => {
    if (mediaRecorder && mediaRecorder.state === 'recording') {
        updateSubmitButton('manual');
        clearTimeout(recordingTimeout);
        mediaRecorder.stop();
    }
});

// Handle page unload to clean up
window.addEventListener('beforeunload', () => {
    if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
    }
    if (recordingStartTimeout) {
        clearTimeout(recordingStartTimeout);
    }
    if (recordingTimeout) {
        clearTimeout(recordingTimeout);
    }
});

// Start with the first video
videoElement.src = videos.intro;
videoElement.autoplay = true;
videoElement.loop = false;
videoElement.controls = false;

// Prevent users from pausing video with keyboard or other methods
videoElement.addEventListener('pause', () => {
    if (currentStage === 'intro' || 
        currentStage === 'speaking' || 
        currentStage === 'followup') {
        videoElement.play();
    }
});

// Prevent seeking (skipping ahead)
videoElement.addEventListener('seeking', () => {
    videoElement.currentTime = videoElement.dataset.lastTime || 0;
});

videoElement.addEventListener('timeupdate', () => {
    videoElement.dataset.lastTime = videoElement.currentTime;
});

console.log('Recording interface initial display:', recordingInterface.style.display);
</script>
{% endblock %}